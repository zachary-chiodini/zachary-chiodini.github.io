<!DOCTYPE html>
<html>
  <head>
    <meta prefix="og: http://ogp.me/ns#" property="og:type" content="website" />
    <meta prefix="og: http://ogp.me/ns#" property="og:title" content="The Perceptron Unit" />
    <meta prefix="og: http://ogp.me/ns#" property="og:description" content="This page presents a description of a single perceptron unit and its applications." />
    <meta prefix="og: http://ogp.me/ns#" property="og:image" content="perceptron_learning.png" />
    <meta prefix="og: http://ogp.me/ns#" property="og:url" content="https://zachary-chiodini.github.io/perceptron/perceptron.html?x=153&y=57" />
  </head>
  <title>The Perceptron</title>
  <body style="background-color:#002240; color:white; padding:30px">
    <h1 style="color:#02FF02">The Perceptron Unit</h1>
    <p>
      This page serves as a learning tool to assist in understanding a single perceptron unit and its applications,
      outside of multilayered nueral networks, with C++ and Python.
      Multilayered neural networks are the talk of the town. 
      However, let's not get ahead of ourselves.
      Instead, let's start with a discussion of the units within the network, perceptrons.
      Once the perceptron is understood, a neural network can follow.
    </p>
    <p>
      This page is not meant to be an exhaustive source of perceptron knowledge.
      It is meant to be breif and straight to the point.
      Additionally, this page may serve as a breif introductory comparison between C++ and Python.
    </p>
    <h1 style="color:#02FF02">Mathematical Model</h1>
    <p>
      Let's describe a perceptron by a function f.
      The mathematical model for a typical perceptron is a simple step function (1), where τ is called the threshold.
      The function can be rewritten so that the inequalities are set to zero (2).
      In this configuration, the threshold τ is often referred to as the bias b, where b = -τ.
      This is also called a boolean function, because its output can only be either 1 or 0, which corresponds to true or false.
      A graph of the step function is shown to the right. It's no coincidence that it looks like a step.
    </p>
    <table align="center" style="border-spacing:0px" >
      <tr>
        <td><table align="center" style="border-spacing:0px">
          <tr><td><img style="width:75%; height:75%" src="step_function.png" /></td></tr>
          <tr><td><img style="width:75%; height:75%" src="step_function_2.png" /></td></tr>
        </table></td>
        <td><img style="width:75%; height:75%"; src="step_graph.png" /></td>
      </tr>
    </table
    <p>
      The perceptron is really that simple, but the function is hiding some important details.
      The input x for the perceptron is a weighted sum, which is given by the dot product of two vectors: 
      an inpute vector x and the network vector ω, each having n components (3).
      The input vector contains the data to be interpreted by the perceptron. 
      The network vector contains the weight or coefficient for each input vector component.
      To make things more compact, we can increase the size of both vectors to n + 1 and 
      force the first component of the input vector to be -1
      and the first component of the network vector to be the threshold τ,
      so that the first product in the sum is -τ.
    </p>
  </body>
</html>
