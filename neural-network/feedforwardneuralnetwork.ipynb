{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#F4EE00; font-family:cambria\"><i>Multi-Layered, Feedforward Neural Network</i></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>This is a multi-layred, feedforward neural network written from scratch in Python.</i>\n",
    "</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"neuralnetwork.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4EE00; font-family:cambria\"><i>Importing Libraries</i></h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, List, Tuple, TypedDict, Union\n",
    "from nptyping import NDArray, Float64\n",
    "np.seterr( over = 'raise' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#F4EE00; font-family:cambria\"><i>Defining Data Structures</i></h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Input Matrix</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{blue}{X} = \\begin{bmatrix}\n",
    "    \\color{blue}{x_{1,1}} & \\color{blue}{x_{1,2}} & \\dots & \\color{blue}{x_{1,n}} \\\\\n",
    "    \\color{blue}{x_{2,1}} & \\color{blue}{x_{2,2}} & \\dots & \\color{blue}{x_{2,n}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\color{blue}{x_{m,1}} & \\color{blue}{x_{m,2}} & \\dots & \\color{blue}{x_{m,n}} \\\\\n",
    "\\end{bmatrix}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples = Any\n",
    "Features = Any\n",
    "Input_Matrix = NDArray[ ( Examples, Features ), Float64 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Target Matrix</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{yellow}{Y} = \\begin{bmatrix}\n",
    "    \\color{yellow}{y_{1,1}} & \\color{yellow}{y_{1,2}} & \\dots & \\color{yellow}{y_{1,k}} \\\\\n",
    "    \\color{yellow}{y_{2,1}} & \\color{yellow}{y_{2,2}} & \\dots & \\color{yellow}{y_{2,k}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\color{yellow}{y_{m,1}} & \\color{yellow}{y_{m,2}} & \\dots & \\color{yellow}{y_{m,k}}\n",
    "\\end{bmatrix}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Targets = Any\n",
    "Target_Matrix = NDArray[ ( Examples, Targets ), Float64 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Output</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{yellow}{\\hat{Y}} = \\begin{bmatrix}\n",
    "    \\color{yellow}{\\hat{y}_{1,1}} & \\color{yellow}{\\hat{y}_{1,2}} & \\dots & \\color{yellow}{\\hat{y}_{1,k}} \\\\\n",
    "    \\color{yellow}{\\hat{y}_{2,1}} & \\color{yellow}{\\hat{y}_{2,2}} & \\dots & \\color{yellow}{\\hat{y}_{2,k}} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\color{yellow}{\\hat{y}_{m,1}} & \\color{yellow}{\\hat{y}_{m,2}} & \\dots & \\color{yellow}{\\hat{y}_{m,k}}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_Matrix = NDArray[ ( Examples, Targets ), Float64 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Perceptron</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{green}{P}^{(ℓ)}_{\\color{green}{k}} = \\begin{bmatrix}\n",
    "    \\color{green}{\\omega}^{(ℓ)}_{\\color{green}{1,k}} \\\\\n",
    "    \\color{green}{\\omega}^{(ℓ)}_{\\color{green}{2,k}} \\\\\n",
    "    \\vdots \\\\ \n",
    "    \\color{green}{\\omega}^{(ℓ)}_{\\color{green}{j,k}}\n",
    "\\end{bmatrix} \\\\\n",
    "\\color{green}{b}^{(ℓ)}_{\\color{green}{k}} = \\color{green}{\\omega}^{(ℓ)}_{\\color{green}{0,k}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inputs = Any\n",
    "Bias, Weight = Float64, Float64\n",
    "Perceptron = NDArray[ ( Inputs, 1 ), Weight ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Layer</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{green}{\\Omega}^{(ℓ)} = \\begin{bmatrix}\n",
    "    \\color{green}{P}^{(ℓ)}_{\\color{green}{1}} & \n",
    "    \\color{green}{P}^{(ℓ)}_{\\color{green}{2}} & \n",
    "    \\dots & \n",
    "    \\color{green}{P}^{(ℓ)}_{\\color{green}{k}}\n",
    "\\end{bmatrix} \\\\\n",
    "\\color{green}{\\beta}^{(ℓ)} = \\begin{bmatrix}\n",
    "    \\color{green}{b}^{(ℓ)}_{\\color{green}{1}} & \n",
    "    \\color{green}{b}^{(ℓ)}_{\\color{green}{2}} & \n",
    "    \\dots & \n",
    "    \\color{green}{b}^{(ℓ)}_{\\color{green}{k}}\n",
    "\\end{bmatrix}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Perceptrons = Any\n",
    "Weights = NDArray[ ( Inputs, Number_of_Perceptrons ), Perceptron ]\n",
    "Biases = NDArray[ Number_of_Perceptrons, Bias ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Neural Network</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{green}{N} = \\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "       \\color{green}{\\Omega}^{(1)} &\n",
    "       \\color{green}{\\Omega}^{(2)} & \n",
    "       \\dots & \n",
    "       \\color{green}{\\Omega}^{(ℓ)}\n",
    "   \\end{bmatrix} & \n",
    "   \\begin{bmatrix}\n",
    "       \\color{green}{\\beta}^{(1)} &\n",
    "       \\color{green}{\\beta}^{(2)} & \n",
    "       \\dots & \n",
    "       \\color{green}{\\beta}^{(ℓ)}\n",
    "   \\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network_Weights = Dict[ int, Weights ]\n",
    "Network_Biases = Dict[ int, Biases ]\n",
    "class Network( TypedDict ) :\n",
    "    weights : Network_Weights\n",
    "    biases  : Network_Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#F4EE00; font-family:cambria\"><i>Creating the Neural Network</i></h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Initialization</i>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork :\n",
    "    \n",
    "    def __init__( self, perceptrons_per_hidden_layer : List[ int ] = [] ) -> None :\n",
    "        self.score = 0.0\n",
    "        self.perlayer = perceptrons_per_hidden_layer\n",
    "        self.network : Network = { 'weights' : {}, 'biases' : {} }\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Initialize Random Weights and Biases</i>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def initialize( self, X : Input_Matrix, Y : Target_Matrix ) -> None :\n",
    "        inputs, layer = X.shape[ 1 ], 1\n",
    "        self.network = { 'weights' : {}, 'biases' : {} }\n",
    "        for perceptrons in self.perlayer :\n",
    "            self.network[ 'weights' ][ layer ] = np.random.rand( inputs, perceptrons )\n",
    "            self.network[ 'biases' ][ layer ] = np.random.rand( perceptrons )\n",
    "            inputs = perceptrons\n",
    "            layer += 1\n",
    "        self.network[ 'weights' ][ layer ] = np.random.rand( inputs, Y.shape[ 1 ] )\n",
    "        self.network[ 'biases' ][ layer ] = np.random.rand( Y.shape[ 1 ] )\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Activation Function</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:14px\">$\\color{blue}{Sigmoid}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\n",
    "        \\color{pink}{f}(x) = \\frac{1}{1+\\color{blue}{e}^{-x}}\n",
    "        $</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:14px\">$\\color{blue}{Derivative}$</td>\n",
    "        <td style=\"font-size:24px\">$\n",
    "        \\color{pink}{f}(x) = \\frac{\\color{blue}{e}^{-x}}{(1+\\color{blue}{e}^{-x})^{2}}\n",
    "        $</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def activation( self, L : Input_Matrix ) -> Union[ NDArray[ Float64 ], Float64 ] :\n",
    "        return 1.0 / ( 1.0 + np.exp( -L ) )\n",
    "    \n",
    "    def derivative( self, L : Input_Matrix ) -> Union[ NDArray[ Float64 ], Float64 ] :\n",
    "        return np.exp( -L ) / np.square( 1.0 + np.exp( -L ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Cost and Gradient</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:14px\">$\\color{blue}{Cost\\ Function}$</td>\n",
    "        <td style=\"font-size:24px\">$\n",
    "        \\color{pink}{C}(\\color{yellow}{Y};\\color{yellow}{\\hat{Y}}) = \n",
    "        \\sum_{h=1}^{m}{\\sum_{i=1}^{k} (\\color{yellow}{\\hat{y}}_{h,i} - \\color{yellow}{y}_{h,i}})^{2} =\n",
    "        \\sum_{h=1}^{m}{\\sum_{i=1}^{k} (\\color{pink}{a}^{(ℓ)}_{h,i} - \\color{yellow}{y}_{h,i}})^{2} \n",
    "        $</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:14px\">$\\color{blue}{Gradient}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\n",
    "        \\nabla^{(ℓ)}_{\\color{pink}{a}}\\color{pink}{C} = \\begin{bmatrix}\n",
    "                \\frac{\\partial \\color{pink}{C}}{\\partial \\color{pink}{a}^{(ℓ)}_{\\color{pink}{1,1}}} &\n",
    "                \\frac{\\partial \\color{pink}{C}}{\\partial \\color{pink}{a}^{(ℓ)}_{\\color{pink}{1,2}}} &\n",
    "                \\dots & \n",
    "                \\frac{\\partial \\color{pink}{C}}{\\partial \\color{pink}{a}^{(ℓ)}_{\\color{pink}{1,k}}} \n",
    "                \\\\\n",
    "                \\frac{\\partial \\color{pink}{C}}{\\partial \\color{pink}{a}^{(ℓ)}_{\\color{pink}{2,1}}} &\n",
    "                \\frac{\\partial \\color{pink}{C}}{\\partial \\color{pink}{a}^{(ℓ)}_{\\color{pink}{2,2}}} &\n",
    "                \\dots & \n",
    "                \\frac{\\partial \\color{pink}{C}}{\\partial \\color{pink}{a}^{(ℓ)}_{\\color{pink}{2,k}}}\n",
    "                \\\\\n",
    "                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                \\\\\n",
    "                \\frac{\\partial \\color{pink}{C}}{\\partial \\color{pink}{a}^{(ℓ)}_{\\color{pink}{m,1}}} &\n",
    "                \\frac{\\partial \\color{pink}{C}}{\\partial \\color{pink}{a}^{(ℓ)}_{\\color{pink}{m,2}}} &\n",
    "                \\dots & \n",
    "                \\frac{\\partial \\color{pink}{C}}{\\partial \\color{pink}{a}^{(ℓ)}_{\\color{pink}{m,k}}}\n",
    "            \\end{bmatrix} = 2(\\color{yellow}{\\hat{Y}}-\\color{yellow}{Y}) \n",
    "        $</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def cost( self, A : Output_Matrix, Y : Target_Matrix ) -> Float64 :\n",
    "        return np.square( A - Y ).sum()\n",
    "    \n",
    "    def grad( self, A : Output_Matrix, Y : Target_Matrix ) -> NDArray :\n",
    "        return 2*( A - Y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Forward Propagation</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:14px\">$\\color{blue}{Output\\ and\\ Hidden\\ Layer}$</td>\n",
    "        <td style=\"font-size:24px\">$\n",
    "            \\color{blue}{L}^{(ℓ)}=\n",
    "            \\color{red}{f}⊙(\\color{blue}{L}^{(ℓ-1)}\\color{green}{\\Omega}^{ℓ} + \\color{green}{\\beta}^{ℓ})\n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:14px\">$\\color{blue}{Input Layer}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">$\\color{blue}{L}^{(0)} = \\color{blue}{X}$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def forwardpropagation( self, L : Input_Matrix ) -> Output_Matrix :\n",
    "        for layer in range( 1, len( self.network[ 'weights' ] ) + 1 ) :\n",
    "            L = self.activation( \n",
    "                np.matmul( L, self.network[ 'weights' ][ layer ] ) \\\n",
    "                + self.network[ 'biases' ][ layer ] \n",
    "                ) \n",
    "        return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Backpropagation</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:14px\">$\\color{blue}{Containers}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $\n",
    "            \\color{orange}{Z}[ℓ] = \\color{blue}{L}^{(ℓ-1)}\\color{green}{\\Omega}^{(ℓ)}+\\color{green}{\\beta}^{(ℓ)} \\\\\n",
    "            \\color{pink}{A}[ℓ] = \\color{blue}{L}^{(ℓ)}\n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def __forwardpropagation( self, X : Input_Matrix, A : Dict, Z : Dict ) -> Output_Matrix :\n",
    "        A[ 0 ] = X\n",
    "        for layer in range( 1, len( self.network[ 'weights' ] ) + 1 ) :\n",
    "            # weighted input to layer\n",
    "            Z[ layer ] = np.matmul( A[ layer - 1 ], self.network[ 'weights' ][ layer ] ) \\\n",
    "                         + self.network[ 'biases' ][ layer ]\n",
    "            # output of layer\n",
    "            A[ layer ] = self.activation( Z[ layer ] )\n",
    "        return A[ layer ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Backpropagation</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td style=\"font-size:14px\">$\\color{blue}{Initialization}$</td>\n",
    "        <td style=\"font-size:24px; text-align:left\">\n",
    "            $\n",
    "            \\nabla^{(ℓ)}_{\\color{orange}{z}}\\color{pink}{C}=\n",
    "            (\\color{pink}{f}'⊙\\color{orange}{Z}[ℓ])⊙\\nabla^{(ℓ)}_{\\color{pink}{a}}\\color{pink}{C}\n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"font-size:14px\">$\\color{blue}{Output\\ and\\ Hidden\\ Layer}$</td>\n",
    "        <td style=\"font-size:24px\">\n",
    "            $\n",
    "            \\nabla^{(ℓ)}_{\\color{green}{\\Omega}}\\color{pink}{C}=\n",
    "            \\color{pink}{A}[ℓ-1]^{T}\\nabla^{(ℓ)}_{\\color{orange}{z}}\\color{pink}{C} \\\\\n",
    "            \\nabla^{(ℓ)}_{\\color{green}{\\beta}}\\color{pink}{C}=\n",
    "            1_{1\\times m}\\nabla^{(ℓ)}_{\\color{orange}{z}}\\color{pink}{C} \\\\\n",
    "            \\nabla^{(ℓ-1)}_{\\color{orange}{z}}\\color{pink}{C}=\n",
    "            (\\color{pink}{f}'⊙\\color{orange}{Z}[ℓ-1])⊙\\nabla^{(ℓ)}_{\\color{orange}{z}}\\color{pink}{C}(\\color{green}{\\Omega}^{(ℓ)})^{T}\n",
    "            $\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def backpropagate( self, grad_z : NDArray, A : Dict, Z : Dict, layer : int ) -> Tuple :\n",
    "        # gradient with respect to the weights of the layer\n",
    "        grad_w = np.matmul( A[ layer - 1 ].T, grad_z ) / len( grad_z )\n",
    "        # gradient with respect to the biases of the layer\n",
    "        grad_b = grad_z.mean( axis = 0 )\n",
    "        # gradient with respect to the weighted input of the layer\n",
    "        if layer > 1 : # there is no weighted input for layer 1\n",
    "            grad_z = self.derivative( Z[ layer - 1 ] ) *\\\n",
    "                     np.matmul( grad_z, self.network[ 'weights' ][ layer ].T )\n",
    "        return grad_z, grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>Training Algorithm</i>\n",
    "</span>\n",
    "<br>\n",
    "<br>\n",
    "<span style=\"font-size:24px\">$\n",
    "\\color{green}{\\Omega}^{(ℓ)} \\rightarrow \\color{green}{\\Omega}^{(ℓ)} - r\\nabla^{(ℓ)}_{\\color{green}{\\Omega}}\\color{pink}{C} \\\\\n",
    "\\color{green}{\\beta}^{(ℓ)} \\rightarrow \\color{green}{\\beta}^{(ℓ)} - r\\nabla^{(ℓ)}_{\\color{green}{\\beta}}\\color{pink}{C}\n",
    "$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork( FeedForwardNeuralNetwork  ) :\n",
    "    \n",
    "    def train( self, X : Input_Matrix, Y : Target_Matrix, \n",
    "               learning_rate = 1.0, convergence = 0.01, \n",
    "               batch_size = 100, max_epoch = 500, max_time = 60 ) -> None :\n",
    "        ''' Stochastic Gradient Descent '''\n",
    "        epoch = 1\n",
    "        A, Z = {}, {}\n",
    "        start = time()\n",
    "        totgrad = np.inf\n",
    "        self.initialize( X, Y )\n",
    "        output_layer = len( self.network[ 'weights' ] )\n",
    "        while np.sqrt( totgrad ) > convergence :\n",
    "            totgrad = 0\n",
    "            shuffle = np.random.permutation( len( X ) )\n",
    "            X, Y = X[ shuffle ], Y[ shuffle ]\n",
    "            for batch_x, batch_y in zip( \n",
    "                np.array_split( X, len( X ) // batch_size ), \n",
    "                np.array_split( Y, len( Y ) // batch_size ) \n",
    "                ) :\n",
    "                output = self.__forwardpropagation( batch_x, A, Z )\n",
    "                # gradient with respect to the output layer\n",
    "                grad_a = self.grad( output, batch_y )\n",
    "                totgrad += np.linalg.norm( grad_a )**2\n",
    "                # gradient with respect to the weighted input of the output layer\n",
    "                grad_z = self.derivative( Z[ output_layer ] ) * grad_a\n",
    "                for layer in range( output_layer, 0, -1 ) :\n",
    "                    grad_z, grad_w, grad_b = self.backpropagate( grad_z, A, Z, layer )\n",
    "                    # updating the weights and biases of layer\n",
    "                    self.network[ 'weights' ][ layer ] -= learning_rate * grad_w\n",
    "                    self.network[ 'biases' ][ layer ] -= learning_rate * grad_b\n",
    "            epoch += 1\n",
    "            if time() - start > max_time :\n",
    "                print( 'Maximum runtime encountered.' )\n",
    "                break\n",
    "            if epoch > max_epoch :\n",
    "                print( 'Maximum epoch encountered.' )\n",
    "                break\n",
    "        self.score = self.cost( self.forwardpropagation( X ), Y )\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#F4EE00; font-family:cambria\"><i>Test Drive on MNIST Data!</i></h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt( 'mnist_train.csv', delimiter = ',', skiprows = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train[ :, 1 : ]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.ndarray( shape = ( len( train ), 10 ), dtype = np.float )\n",
    "for i, n in np.ndenumerate( train[ :, 0 ] ) :\n",
    "    vect = np.array( [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ], dtype = np.float )\n",
    "    vect[ int( n ) ] = 1.0\n",
    "    Y[ i ] = vect\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN = FeedForwardNeuralNetwork( perceptrons_per_hidden_layer = [ 30 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum runtime encountered.\n"
     ]
    }
   ],
   "source": [
    "FFNN.train( X, Y, learning_rate = 3, convergence = 0.01, batch_size = 1, max_epoch = 30, max_time = 60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108163.98070654711"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFNN.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#53C8FE; font-family:aparajita; font-size:18px\">\n",
    "    <i>A matrix of identical vectors?</i>\n",
    "</span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00494983e-07, 9.04043460e-10, 6.36043136e-11, ...,\n",
       "        1.68176401e-11, 1.97903819e-11, 1.33130030e-10],\n",
       "       [3.00494983e-07, 9.04043460e-10, 6.36043136e-11, ...,\n",
       "        1.68176401e-11, 1.97903819e-11, 1.33130030e-10],\n",
       "       [3.00494983e-07, 9.04043460e-10, 6.36043136e-11, ...,\n",
       "        1.68176401e-11, 1.97903819e-11, 1.33130030e-10],\n",
       "       ...,\n",
       "       [3.00494983e-07, 9.04043460e-10, 6.36043136e-11, ...,\n",
       "        1.68176401e-11, 1.97903819e-11, 1.33130030e-10],\n",
       "       [3.00494983e-07, 9.04043460e-10, 6.36043136e-11, ...,\n",
       "        1.68176401e-11, 1.97903819e-11, 1.33130030e-10],\n",
       "       [3.00494983e-07, 9.04043460e-10, 6.36043136e-11, ...,\n",
       "        1.68176401e-11, 1.97903819e-11, 1.33130030e-10]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFNN.forwardpropagation( X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.00494983e-07, 9.04043460e-10, 6.36043136e-11, 2.83253745e-11,\n",
       "       9.91102064e-09, 7.48532676e-07, 9.99999931e-01, 1.68176401e-11,\n",
       "       1.97903819e-11, 1.33130030e-10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFNN.forwardpropagation( X )[ 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.00494983e-07, 9.04043460e-10, 6.36043136e-11, 2.83253745e-11,\n",
       "       9.91102064e-09, 7.48532676e-07, 9.99999931e-01, 1.68176401e-11,\n",
       "       1.97903819e-11, 1.33130030e-10])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFNN.forwardpropagation( X )[ 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[55000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.00494983e-07, 9.04043460e-10, 6.36043136e-11, 2.83253745e-11,\n",
       "       9.91102064e-09, 7.48532676e-07, 9.99999931e-01, 1.68176401e-11,\n",
       "       1.97903819e-11, 1.33130030e-10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFNN.forwardpropagation( X )[ 55000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.00494983e-07, 9.04043460e-10, 6.36043136e-11, 2.83253745e-11,\n",
       "       9.91102064e-09, 7.48532676e-07, 9.99999931e-01, 1.68176401e-11,\n",
       "       1.97903819e-11, 1.33130030e-10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFNN.forwardpropagation( X )[ 30000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " # every vector is [ 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#F4EE00; font-family:cambria\"><i>Let's try Nielson's</i></h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print( \"Epoch {0}: {1} / {2}\" ).format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print( \"Epoch {0} complete\" ).format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list( zip( X, Y ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network( [ 784, 30, 10 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FloatingPointError",
     "evalue": "overflow encountered in exp",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFloatingPointError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-df356305646a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-ca77c26712ca>\u001b[0m in \u001b[0;36mSGD\u001b[1;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 for k in range(0, n, mini_batch_size)]\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 print( \"Epoch {0}: {1} / {2}\" ).format(\n",
      "\u001b[1;32m<ipython-input-28-ca77c26712ca>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[1;34m(self, mini_batch, eta)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdnb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_nabla_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mnabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdnw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-ca77c26712ca>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mzs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-ca77c26712ca>\u001b[0m in \u001b[0;36msigmoid\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;34m\"\"\"The sigmoid function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid_prime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFloatingPointError\u001b[0m: overflow encountered in exp"
     ]
    }
   ],
   "source": [
    "network.SGD( train, 30, 10, 0.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>:O'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'>:O'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
